{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bioinformatic File Formats Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Bioinformatics File Formats and Data Lakes Using Glow\n",
    "\n",
    "Bioinformatics often involves the manipulation and analysis of large-scale genomic data, which is typically stored in specialized file formats such as Variant Call Format (VCF) and General Feature Format Version 3 (GFF3). These formats are crucial for representing genetic variants and annotations, respectively, and are commonly used in various genomic studies.\n",
    "\n",
    "VCF files store information about genetic variants, including details such as the chromosome, position, reference, and alternative alleles, as well as associated metadata. GFF3 files, on the other hand, are used to describe gene and other genomic feature annotations, providing a structured way to represent the location and attributes of various genomic elements.\n",
    "\n",
    "To efficiently manage and analyze such vast datasets, especially in distributed computing environments, Apache Spark has emerged as a powerful tool. The Glow library extends Sparkâ€™s capabilities, allowing bioinformaticians to process VCF and GFF3 files at scale with Spark DataFrames. This integration facilitates parallel processing, making it easier to handle large genomic datasets.\n",
    "\n",
    "Moreover, to ensure data is stored in a scalable, reliable, and queryable manner, integrating with Delta Lake is highly advantageous. Delta Lake provides ACID transactions, scalable metadata handling, and efficient file format support. By converting VCF and GFF3 files into Delta format, researchers can leverage these features, enabling efficient data storage, versioning, and querying.\n",
    "\n",
    "This approach allows bioinformatics workflows to be more robust, with better data management and faster processing times, especially when dealing with large datasets. Below, we explore how to manipulate these file formats using Glow and how to store them in Delta Lake for optimal performance and scalability.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "To follow along with this notebook, you need to install the following libraries:\n",
    "```bash\n",
    "pip install biopython pysam pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glow library\n",
    "#### Description\n",
    "ddddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession whit configuration required\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Glow with PySpark\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.projectglow:glow-spark3_2.12:1.2.1,io.delta:delta-core_2.12:2.1.0\") \\\n",
    "    .config(\"spark.hadoop.io.compression.codecs\", \"io.projectglow.sql.util.BGZFCodec\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VCF Format\n",
    "\n",
    "#### Description\n",
    "The VCF (Variant Call Format) is used to store genomic variants such as SNPs, indels, and other structural variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see the structure of the most basic non binary formartmats we can start exploring some good architecture practices to store and process it.\n",
    "\n",
    "Depending of the kind of project we are working, we will need to store and process different genomic formats. Lets supose we have a client which need to store every kind of bioinformatic formats, raw reads (BCL) and processed files (FASTA, FASTQ, GFF3, SAM, VCF).\n",
    "\n",
    "For raw read formats, these can be stored in a data lake using S3, where they can be indexed and partitioned for easier access using patters susch as tenant/year/month/day/file/ which will then be used to process them and run the secondary analyses that will generate the other formats that will allow us to extract information of interest to produce biological insights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BCL to (FASTA or FASTAQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('vcf').load(\"../data/example1000g_chr22_MAF05.vcf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/14 15:07:19 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Row(contigName='22', start=16051248), Row(contigName='17', start=504217))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"contigName\", \"start\").head(), Row(contigName='17', start=504217)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"../data/delta/variants_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107658"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format('delta').load(\"../data/delta/variants_delta\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GFF3 Format\n",
    "\n",
    "#### Description\n",
    "GFF3 (General Feature Format Version 3) is a standard file format used to describe genes and other features of DNA, RNA, and protein sequences. GFF3 files are widely used in genomics for storing annotation data, which includes information about genomic features such as exons, introns, regulatory regions, and more. The format is highly structured and allows for hierarchical relationships between features, making it suitable for complex annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structure of GFF3\n",
    "A typical GFF3 file consists of nine columns, which include:\n",
    "\n",
    "- Seqid: The name of the sequence (e.g., chromosome name).\n",
    "- Source: The source of the annotation (e.g., the name of the program that generated this feature).\n",
    "- Type: The type of feature (e.g., gene, exon, CDS).\n",
    "- Start: The start position of the feature.\n",
    "- End: The end position of the feature.\n",
    "- Score: A floating-point score associated with the feature.\n",
    "- Strand: The strand of the feature (+ or -).\n",
    "- Phase: The phase of the feature (relevant for CDS features).\n",
    "- Attributes: A list of tag-value pairs providing additional information.\n",
    "\n",
    "#### Uses of GFF3\n",
    "GFF3 files are used for:\n",
    "\n",
    "**Genomic Annotation:** Storing the locations and characteristics of genes and other features on genomes.\n",
    "**Comparative Genomics:** Comparing gene annotations across different species.\n",
    "**Bioinformatics Pipelines:** Serving as input for various bioinformatics tools that analyze genomic data.\n",
    "\n",
    "#### Manipulating GFF3 with Glow and Delta Lake\n",
    "Glow is an open-source toolkit for large-scale genomic data analysis on Apache Spark. It extends Spark's capabilities with genomic data types and functions, allowing seamless integration with standard formats like GFF3.\n",
    "\n",
    "Delta Lake is an open-source storage layer that brings reliability to data lakes. It provides ACID transactions, scalable metadata handling, and unified streaming and batch data processing.\n",
    "\n",
    "#### Reading GFF3 with Glow\n",
    "\n",
    "To read a GFF3 file using Glow and Apache Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gff3_df = spark.read.format(\"gff\").load(\"../data/example.gff3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+-----+----+-----+------+-----+-----+-----------+-------+\n",
      "|seqId|source|type|start| end|score|strand|phase|   ID|       Name| Parent|\n",
      "+-----+------+----+-----+----+-----+------+-----+-----+-----------+-------+\n",
      "| seq1|  null|gene|  999|2000| null|     +| null|gene1|      Gene1|   null|\n",
      "| seq1|  null|mRNA| 1049|1800| null|     +| null|mRNA1|Transcript1|[gene1]|\n",
      "| seq1|  null|exon| 1049|1200| null|     +| null|exon1|      Exon1|[mRNA1]|\n",
      "| seq1|  null|exon| 1249|1500| null|     +| null|exon2|      Exon2|[mRNA1]|\n",
      "| seq1|  null|exon| 1549|1800| null|     +| null|exon3|      Exon3|[mRNA1]|\n",
      "| seq1|  null| CDS| 1059|1190| null|     +|    0| cds1|       null|[mRNA1]|\n",
      "| seq1|  null| CDS| 1259|1490| null|     +|    0| cds2|       null|[mRNA1]|\n",
      "| seq1|  null| CDS| 1559|1790| null|     +|    0| cds3|       null|[mRNA1]|\n",
      "+-----+------+----+-----+----+-----+------+-----+-----+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gff3_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[seqId: string, source: string, type: string, start: bigint, end: bigint, score: double, strand: string, phase: int, ID: string, Name: string, Parent: array<string>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Number of records: {gff3_df.count()}\")\n",
    "display(gff3_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl_original_gff_df = gff3_df.select(['start', 'end', 'type', 'seqId', 'source', 'strand', 'phase', 'ID', 'Name', 'Parent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+-----+------+------+-----+-----+-----------+-------+\n",
      "|start| end|type|seqId|source|strand|phase|   ID|       Name| Parent|\n",
      "+-----+----+----+-----+------+------+-----+-----+-----------+-------+\n",
      "|  999|2000|gene| seq1|  null|     +| null|gene1|      Gene1|   null|\n",
      "| 1049|1800|mRNA| seq1|  null|     +| null|mRNA1|Transcript1|[gene1]|\n",
      "| 1049|1200|exon| seq1|  null|     +| null|exon1|      Exon1|[mRNA1]|\n",
      "| 1249|1500|exon| seq1|  null|     +| null|exon2|      Exon2|[mRNA1]|\n",
      "| 1549|1800|exon| seq1|  null|     +| null|exon3|      Exon3|[mRNA1]|\n",
      "| 1059|1190| CDS| seq1|  null|     +|    0| cds1|       null|[mRNA1]|\n",
      "| 1259|1490| CDS| seq1|  null|     +|    0| cds2|       null|[mRNA1]|\n",
      "| 1559|1790| CDS| seq1|  null|     +|    0| cds3|       null|[mRNA1]|\n",
      "+-----+----+----+-----+------+------+-----+-----+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "etl_original_gff_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotate chromosome (contigName) to gff3 dataframe\n",
    "Selecting regions and joining back to original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|seqId|   ID|\n",
      "+-----+-----+\n",
      "| seq1|exon1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regions_df = gff3_df.where((f.col(\"type\") == \"exon\") & (f.col(\"Name\") == \"Exon1\")). \\\n",
    "                             select(\"seqId\", \"ID\")\n",
    "regions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regions_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "gff3_df.write.mode(\"overwrite\").format(\"delta\").save(\"../data/delta/gff3_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/15 17:52:31 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 293180 ms exceeds timeout 120000 ms\n",
      "24/08/15 17:52:31 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('delta').load(\"../data/delta/gff3_delta\").count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
